{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Face Recognition With Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beschreibe Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import cv2 as cv\n",
    "import glob\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import os.path\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# network training\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "n544u": [
       {
        "id": "12668441/AN5HZ4B7",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "- Vorgehen mit OpenCV beschreiben\n",
    "- Random faces database zitieren <cite id=\"n544u\"><a href=\"#zotero%7C12668441%2FAN5HZ4B7\">(Huang et al., 2007)</a></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "First of all a couple of constants are defined to specify the directories which contain the unprocessed image data. Additionally an image size in pixels is defined to compute the images to this size later on. Also the batch size is defined to control how many samples are used in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "IMAGES_PATH = \"images/\"\n",
    "IMAGES_ME_PATH = IMAGES_PATH + \"me/*\"\n",
    "IMAGES_SE_PATH = IMAGES_PATH + \"se\"\n",
    "IMAGES_SE_INPUTPATH = IMAGES_SE_PATH + \"/*/*/*\"\n",
    "IMG_SIZE = 192\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dataset to recognize my own face and clearly discern it from other faces it is necessary to get as much pictures of my face as possible. Furthermore different faces are needed. First it is dealt with getting pictures of myself. To achieve this private pictures from my phone and webcam recordings are saved. Secondly OpenCV is used to detect faces in the pictures. This is especially useful to localize a region of interest if the picture contains multiple faces. The cropped images are getting sorted by hand and saved. This step is already done and does not need to be computed to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cell not executable\n",
    "%%script false\n",
    "\n",
    "# get images from data path\n",
    "images = [cv.imread(file) for file in glob.glob(f\"{IMAGES_PATH}mePreDetection\\*\")]\n",
    "# convert the images to grayscale\n",
    "images_gray = [cv.cvtColor(image, cv.COLOR_BGR2GRAY) for image in images]\n",
    "# using cascade classifier for general face detection\n",
    "cascade = cv.CascadeClassifier(r\"files\\haarcascade_frontalface_default.xml\")\n",
    "# detect faces\n",
    "faces = [cascade.detectMultiScale(img_gray, scaleFactor=1.3, minNeighbors=5, minSize=(100, 100)) for img_gray in\n",
    "         images_gray]\n",
    "\n",
    "# loop over images list\n",
    "j = 0\n",
    "k = 0\n",
    "for i, img in enumerate(images):\n",
    "    for x, y, width, height in faces[i]:\n",
    "        # print rectangle around ROI for debugging\n",
    "        #cv.rectangle(img, (x, y), (x + width, y + height), (0, 0, 255), 1)\n",
    "        # crop image at ROI\n",
    "        crop_image = img[y:y + height, x:x + width]\n",
    "        # show image for manual validation\n",
    "        cv.imshow('cropped image', crop_image)\n",
    "        cv.waitKey(0)\n",
    "        cv.destroyAllWindows()\n",
    "        # ask user to sort image\n",
    "        print(\"Press 'm' if the picture shows your face. Press 's' if the picture shows something or somebody else. \"\n",
    "              \"Press 'q' if you don't want to save the picture\")\n",
    "        decision = input()\n",
    "        if decision == 'm':\n",
    "            saved = cv.imwrite(f\"{IMAGES_PATH}me/me_{j}.jpg\", crop_image)\n",
    "            if saved:\n",
    "                print(f\"saved to {IMAGES_PATH}me/me_{j}.jpg\")\n",
    "            j += 1\n",
    "        elif decision == 's':\n",
    "            saved = cv.imwrite(f\"{IMAGES_PATH}se/se_{k}.jpg\", crop_image)\n",
    "            if saved:\n",
    "                print(f\"saved to {IMAGES_PATH}se/se_{k}.jpg\")\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset\n",
    "To make handling the data easier with tensorflow the *tf.keras.utils.image_dataset_from_directory*-method is used to create a *tf.data.Dataset* from the images in the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random faces database in format for tf dataset creating\n",
    "# if each subdirectory contains images for a class tf assigns labels automatically\n",
    "# get max pictures of se\n",
    "imagesSe = [image.load_img(file) for file in glob.glob(IMAGES_SE_INPUTPATH)]\n",
    "\n",
    "# create tf datasets\n",
    "trainDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    IMAGES_PATH,\n",
    "    labels='inferred',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "validationDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    IMAGES_PATH,\n",
    "    labels='inferred',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "# print classes for validation of the dataset\n",
    "classNames = np.array(trainDataset.class_names)\n",
    "print(f\"The dataset has {classNames.size} classes. Their names are: {classNames}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many batches are available in validation dataset\n",
    "validationBatches = tf.data.experimental.cardinality(validationDataset)\n",
    "testDataset = validationDataset.take(validationBatches // 5)\n",
    "validationDataset = validationDataset.skip(validationBatches // 5)\n",
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validationDataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(testDataset))\n",
    "\n",
    "# visualize the data\n",
    "classNames = trainDataset.class_names\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in trainDataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(classNames[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data\n",
    "- autotune erklären\n",
    "- standardize erklären\n",
    "- data augmentation erklären"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "trainDataset = trainDataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validationDataset = validationDataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Standardize the data\n",
    "# get RGB values from [0, 255] range to [0, 1] to have small input values for the neural network\n",
    "rescaleImagesLayer = layers.Rescaling(1. / 255)\n",
    "# apply rescaling\n",
    "normalizedDataset = trainDataset.map(lambda x, y: (rescaleImagesLayer(x), y))\n",
    "imageBatch, labelsBatch = next(iter(normalizedDataset))\n",
    "# print batch tensor shape for visualization\n",
    "print(imageBatch.shape)\n",
    "\n",
    "# apply data augmentation from source: https://www.tensorflow.org/tutorials/images/data_augmentation#overview\n",
    "dataAugmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad-CAM class activation visualization\n",
    "\n",
    "Vergleichen mit OpenCV zugeschnitten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "<!-- BIBLIOGRAPHY START -->\n",
    "<div class=\"csl-bib-body\">\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/AN5HZ4B7\"></i>Huang, G. B., Ramesh, M., Berg, T., &#38; Learned-Miller, E. (2007). <i>Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments</i> (No. 07–49). University of Massachusetts, Amherst. <a href=\"http://vis-www.cs.umass.edu/lfw/index.html\">http://vis-www.cs.umass.edu/lfw/index.html</a></div>\n",
    "</div>\n",
    "<!-- BIBLIOGRAPHY END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {
    "zotero": {
     "12668441/AN5HZ4B7": {
      "URL": "http://vis-www.cs.umass.edu/lfw/index.html",
      "author": [
       {
        "family": "Huang",
        "given": "Gary B."
       },
       {
        "family": "Ramesh",
        "given": "Manu"
       },
       {
        "family": "Berg",
        "given": "Tamara"
       },
       {
        "family": "Learned-Miller",
        "given": "Erik"
       }
      ],
      "id": "12668441/AN5HZ4B7",
      "issued": {
       "date-parts": [
        [
         "2007",
         10
        ]
       ]
      },
      "number": "07-49",
      "publisher": "University of Massachusetts, Amherst",
      "system_id": "zotero|12668441/AN5HZ4B7",
      "title": "Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments",
      "type": "report"
     }
    }
   }
  },
  "cite2c": {
   "citations": {
    "12668441/5BZPIWDZ": {
     "URL": "https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub",
     "accessed": {
      "date-parts": [
       [
        2022,
        6,
        18
       ]
      ]
     },
     "container-title": "TensorFlow",
     "language": "en",
     "title": "Transfer learning with TensorFlow Hub | TensorFlow Core",
     "type": "webpage"
    }
   }
  },
  "interpreter": {
   "hash": "cb995c900ad59a34af0dc93b724e2723f333de620077d2080309d75bf10295b1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
