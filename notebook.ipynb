{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Face Recognition With Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rico Steinke, 196949, Heilbronn University, rsteinke@stud.hs-heilbronn.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project task is to 'create a neural network to recognize your face and clearly discern it from other faces and objects'. Therefore a dataset specifically for this task should be created. Also transfer learning techniques should be used with the restriction that the VGG architecture may not be used. For this purpose, the procedure for creating a data set is shown first. Then, the preprocessing of the data for the use in training a neural network. Finally, the results of the training will be shown and evaluated.\n",
    "The notebook comes with a file 'condatf.yml' which can be used to create a conda environment that is able to run this notebook. Every cited source is listed in the [Bibliography](#Bibliography) at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Import the required packages to execute this jupyter notebook and check if a GPU is available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os.path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import ipywidgets as wg\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "n544u": [
       {
        "id": "12668441/AN5HZ4B7",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "- Vorgehen mit OpenCV beschreiben\n",
    "- Random faces database zitieren <cite id=\"n544u\"><a href=\"#zotero%7C12668441%2FAN5HZ4B7\">(Huang et al., 2007)</a></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "First of all a couple of constants are defined to specify the directories which contain the unprocessed image data. Additionally an image size in pixels is defined to compute the images to this size later on. Also the batch size is defined to control how many samples are used in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "IMAGES_PATH = \"images/\"\n",
    "DATASET_PATH = \"dataset/\"\n",
    "DATASET_ME_PATH = DATASET_PATH + \"me\"\n",
    "DATASET_SE_PATH = DATASET_PATH + \"se\"\n",
    "DATASET_RO_PATH = DATASET_PATH + \"ro\"\n",
    "IMAGES_ME_PATH = IMAGES_PATH + \"me\"\n",
    "IMAGES_SE_PATH = IMAGES_PATH + \"se\"\n",
    "IMAGES_RO_PATH = IMAGES_PATH + \"ro\"\n",
    "IMAGES_SE_INPUTPATH = IMAGES_SE_PATH + \"/*/*/*\"\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "mofvs": [
       {
        "id": "12668441/EXM36PCP",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "To create a dataset to recognize my own face and clearly discern it from other faces it is necessary to get as much pictures of my face as possible. Furthermore different faces are needed. First it is dealt with getting pictures of myself. To achieve this private pictures from my phone and webcam recordings are saved. Secondly OpenCV <cite id=\"mofvs\"><a href=\"#zotero%7C12668441%2FEXM36PCP\">(Bradski, 2000)</a></cite> is used to detect faces in the pictures. This is especially useful to localize a region of interest if the picture contains multiple faces. The cropped images are getting sorted by hand and saved. This step is already done and does not need to be computed to run this notebook. If you still want to perform this step, it is recommended to use an alternative IDE with an integrated terminal and copy this cell to it to simplify the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cell not executable\n",
    "%%script false\n",
    "\n",
    "import cv2 as cv\n",
    "\n",
    "# get images from data path\n",
    "images = [cv.imread(file) for file in glob.glob(f\"{IMAGES_PATH}preDetection/*\")]\n",
    "# convert the images to grayscale\n",
    "images_gray = [cv.cvtColor(image, cv.COLOR_BGR2GRAY) for image in images]\n",
    "# using cascade classifier for general face detection\n",
    "cascade = cv.CascadeClassifier(r\"files\\haarcascade_frontalface_default.xml\")\n",
    "# detect faces\n",
    "faces = [\n",
    "    cascade.detectMultiScale(img_gray, scaleFactor=1.3, minNeighbors=5, minSize=(100, 100))\n",
    "    for img_gray in images_gray\n",
    "]\n",
    "\n",
    "# loop over images list\n",
    "j = 0\n",
    "k = 0\n",
    "for i, img in enumerate(images):\n",
    "    for x, y, width, height in faces[i]:\n",
    "        # crop image at ROI\n",
    "        crop_image = img[y : y + height, x : x + width]\n",
    "        # show image for manual validation\n",
    "        cv.imshow(\"cropped image\", crop_image)\n",
    "        cv.waitKey(0)\n",
    "        cv.destroyAllWindows()\n",
    "        # ask user to sort image\n",
    "        print(\n",
    "            \"Press 'm' if the picture shows your face. \"\n",
    "            \"Press 's' if the picture shows something or somebody else. \"\n",
    "            \"Press 'q' if you don't want to save the picture\"\n",
    "        )\n",
    "        decision = input()\n",
    "        if decision == \"m\":\n",
    "            saved = cv.imwrite(f\"{IMAGES_PATH}me/me_{j}.jpg\", crop_image)\n",
    "            if saved:\n",
    "                print(f\"saved to {IMAGES_PATH}me/me_{j}.jpg\")\n",
    "            j += 1\n",
    "        elif decision == \"s\":\n",
    "            saved = cv.imwrite(f\"{IMAGES_SE_PATH}/se_{k}.jpg\", crop_image)\n",
    "            if saved:\n",
    "                print(f\"saved to {IMAGES_SE_PATH}/se_{k}.jpg\")\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZELLE UNTEN AKTUALISIEREN"
   ]
  },
  {
   "attachments": {
    "47be7f07-b481-4298-9414-082eb5338ed6.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folder structure\n",
    "The root folder of this project contains the dataset in the *images*-directory and a *preDetection*-directory which contains the private images before labeling them manually as it is described in the step before. The *me*-directory contains images of my face and the *se*-directory contains images of somebody/something else. Those are the directories which are used to create the dataset described in the next step.\n",
    "\n",
    "**BILD AKTUALISIEREN**\n",
    "\n",
    "![notebookImages\\folderStructure.png](attachment:47be7f07-b481-4298-9414-082eb5338ed6.png)\n",
    "\n",
    "The folder structure used in this project work. Use *'tree /f > tree.txt'* in a shell in your root folder to obtain a graphical view of the folder structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "8avrg": [
       {
        "id": "12668441/DW3K5WQH",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "#### Create Dataset <a id='create_dataset'></a>\n",
    "To make handling the data easier with tensorflow the *tf.keras.utils.image_dataset_from_directory*-method is used to create a *tf.data.Dataset* from the images in the directories. The creation and visualization of the dataset is done as mentioned in the 'Load and preprocess images'-tutorial of TensorFlow <cite id=\"8avrg\"><a href=\"#zotero%7C12668441%2FDW3K5WQH\">(<i>Load and Preprocess Images | TensorFlow Core</i>, n.d.)</a></cite> and fitted to this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let user decide if a new dataset should be created\n",
    "createNewDatasetCb = wg.Checkbox(value=True, description='Create new dataset')\n",
    "display(createNewDatasetCb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random faces database in format for tf dataset creating\n",
    "# if each subdirectory contains images for a class tf assigns labels automatically\n",
    "\n",
    "# get amount of pictures of se\n",
    "amountImagesSe = sum([len(files) for r, d, files in os.walk(IMAGES_SE_PATH)])\n",
    "print(f\"Total amount of pictures of somebody/something else: {amountImagesSe}\")\n",
    "# get max pictures of me\n",
    "amountImagesMe = sum([len(files) for r, d, files in os.walk(IMAGES_ME_PATH)])\n",
    "print(f\"Total amount of pictures of me: {amountImagesMe}\")\n",
    "# get max pictures of ro\n",
    "amountImagesRo = sum([len(files) for r, d, files in os.walk(IMAGES_RO_PATH)])\n",
    "print(f\"Total amount of pictures of ro: {amountImagesRo}\")\n",
    "\n",
    "# let user decide how many images from each class should be used to create the dataset#\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"Use the sliders to control the amount of images used to create the dataset for each class. (You can use the arrow keys of your keyboard to control the sliders more precisely than with the mouse)\")\n",
    "print(\"-------------------------------------------------\")\n",
    "SeSlider = wg.IntSlider(value=amountImagesSe//2, min=0, max=amountImagesSe, description=\"How many images of the class 'se' should be used to create the dataset\")\n",
    "MeSlider = wg.IntSlider(value=amountImagesMe//2, min=0, max=amountImagesMe, description=\"How many images of the class 'me' should be used to create the dataset\")\n",
    "RoSlider = wg.IntSlider(value=amountImagesRo//2, min=0, max=amountImagesRo, description=\"How many images of the class 'ro' should be used to create the dataset\")\n",
    "\n",
    "display(SeSlider, MeSlider, RoSlider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amountImagesSeUserInput = SeSlider.value\n",
    "amountImagesMeUserInput = MeSlider.value\n",
    "amountImagesRoUserInput = RoSlider.value\n",
    "\n",
    "createNewDataset = createNewDatasetCb.value\n",
    "\n",
    "if createNewDataset == True:\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Use {amountImagesSeUserInput} images of se\")\n",
    "    print(f\"Use {amountImagesMeUserInput} images of me\")\n",
    "    print(f\"Use {amountImagesRoUserInput} images of ro\")\n",
    "    print(f\"This makes a total of {amountImagesSeUserInput + amountImagesMeUserInput + amountImagesRoUserInput} images used.\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    # return all path to images in subdirectories from a given root folder\n",
    "    def load_images_from_folder(rootFolder):\n",
    "        images = []\n",
    "        for path, _, files in os.walk(rootFolder):\n",
    "            for name in files:\n",
    "                img = os.path.join(path,name)\n",
    "                if img is not None:\n",
    "                    images.append(img)\n",
    "        return images\n",
    "\n",
    "    imagePathsMe = load_images_from_folder(IMAGES_ME_PATH)\n",
    "    imagePathsSe = load_images_from_folder(IMAGES_SE_PATH)\n",
    "    imagePathsRo = load_images_from_folder(IMAGES_RO_PATH)\n",
    "\n",
    "    # shuffle image lists randomly\n",
    "    random.shuffle(imagePathsMe)\n",
    "    random.shuffle(imagePathsSe)\n",
    "    random.shuffle(imagePathsRo)\n",
    "\n",
    "    # limit amount of images used to create dataset\n",
    "    imagePathsMe = imagePathsMe[:amountImagesMeUserInput]\n",
    "    imagePathsSe = imagePathsSe[:amountImagesSeUserInput]\n",
    "    imagePathsRo = imagePathsRo[:amountImagesRoUserInput]\n",
    "\n",
    "    # create dataset directory\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        shutil.rmtree(DATASET_PATH)\n",
    "\n",
    "    os.makedirs(DATASET_ME_PATH) \n",
    "    os.makedirs(DATASET_SE_PATH)\n",
    "    os.makedirs(DATASET_RO_PATH)\n",
    "    print(f\"Created dataset directory at {DATASET_ME_PATH}, {DATASET_SE_PATH} and {DATASET_RO_PATH}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    # copy random images to dataset directory\n",
    "    [shutil.copy(image, DATASET_ME_PATH) for image in imagePathsMe]\n",
    "    [shutil.copy(image, DATASET_SE_PATH) for image in imagePathsSe]\n",
    "    [shutil.copy(image, DATASET_RO_PATH) for image in imagePathsRo]\n",
    "\n",
    "    # create tf datasets\n",
    "    trainDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        DATASET_PATH,\n",
    "        labels=\"inferred\",\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    validationDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        DATASET_PATH,\n",
    "        labels=\"inferred\",\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "else:\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"No new dataset was created.\")\n",
    "    \n",
    "# print classes for validation of the dataset\n",
    "classNames = np.array(trainDataset.class_names)\n",
    "print(\"-------------------------------------------------\")\n",
    "print(f\"The dataset has {classNames.size} classes. Their names are: {classNames}\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "# determine how many batches are available in validation dataset\n",
    "validationBatches = tf.data.experimental.cardinality(validationDataset)\n",
    "testDataset = validationDataset.take(validationBatches // 5)\n",
    "validationDataset = validationDataset.skip(validationBatches // 5)\n",
    "print(\"Number of validation batches: %d\" % tf.data.experimental.cardinality(validationDataset))\n",
    "print(\"Number of test batches: %d\" % tf.data.experimental.cardinality(testDataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in trainDataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(classNames[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data distribution\n",
    "amountTrainImages = (amountImagesSeUserInput + amountImagesMeUserInput + amountImagesRoUserInput) * (1 - VALIDATION_SPLIT) + 1 # files used for trainig gets alway rounded up\n",
    "amountValidationImages = (amountImagesSeUserInput + amountImagesMeUserInput + amountImagesRoUserInput) * VALIDATION_SPLIT\n",
    "\n",
    "datasetDistribution = [amountTrainImages, amountValidationImages]\n",
    "datasetDistributionLabels = [\"Training Set\", \"Validation Set\"]\n",
    "testDistribution = [tf.data.experimental.cardinality(validationDataset), tf.data.experimental.cardinality(testDataset)]\n",
    "testDistributionLabels = [\"# Validation Batches\", \"# Test Batches\"]\n",
    "\n",
    "def autopct(values):\n",
    "    return '{:.0f} / {:,.0f}%'.format(values, values)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].pie(datasetDistribution, labels=datasetDistributionLabels, radius=2, wedgeprops={\"linewidth\": 2, \"edgecolor\": \"white\"}, frame=False, autopct=autopct(datasetDistribution))\n",
    "axs[1].pie(testDistribution, labels=testDistributionLabels, radius=2, wedgeprops={\"linewidth\": 2, \"edgecolor\": \"white\"}, frame=False, autopct='%.d')\n",
    "axs[0].set_title(\"Training/Validation Split\")\n",
    "axs[1].set_title(\"Validation/Test Split\")\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=2, \n",
    "                    top=2, \n",
    "                    wspace=2, \n",
    "                    hspace=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "ijgfh": [
       {
        "id": "12668441/TCJXDMGP",
        "source": "zotero"
       }
      ],
      "kj5s4": [
       {
        "id": "12668441/MVDHUU4A",
        "source": "zotero"
       }
      ],
      "p8s8c": [
       {
        "id": "12668441/K8WZ388C",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "#### Preprocess the data\n",
    "First of all the dataset is configured for better performance by using the prefetching method. Prefetching overlaps opening, reading and training during the execution of a model. While the training is executed the input pipeline is reading the the data for the next training step. <cite id=\"ijgfh\"><a href=\"#zotero%7C12668441%2FTCJXDMGP\">(<i>Better Performance with the Tf.Data API | TensorFlow Core</i>, n.d.)</a></cite> \n",
    "\n",
    "# Herausfinden wie viel Unterschied Prefetching macht\n",
    "\n",
    "To fit the data for putting it into a neural network it needs to be standardized. This means the image size is fixed for the whole dataset. This step is already done in the ['Create Dataset'](#create-dataset)-section. Additionally the pixel values must be normalized from RGB-range (0 - 255) to fit the pixel space the network expects. In this case the range is between -1 and 1. <cite id=\"kj5s4\"><a href=\"#zotero%7C12668441%2FMVDHUU4A\">(<i>Transfer Learning and Fine-Tuning | TensorFlow Core</i>, n.d.)</a></cite>\n",
    "\n",
    "Data augmentation is a technique to increase the diversification of a data set when using a small data set. For this purpose, the training set is transformed randomly but realistically. <cite id=\"p8s8c\"><a href=\"#zotero%7C12668441%2FK8WZ388C\">(<i>Data Augmentation | TensorFlow Core</i>, n.d.)</a></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "trainDataset = trainDataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validationDataset = validationDataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Normalize and rescale the data\n",
    "preprocessInputMobileNetV2 = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "preprocessInputXception = tf.keras.applications.xception.preprocess_input\n",
    "rescale = tf.keras.layers.Rescaling(1. / 127.5, offset=-1)\n",
    "\n",
    "# print batch tensor shape for validation\n",
    "imageBatch, labelsBatch = next(iter(trainDataset))\n",
    "print(f\"Shape of batches: {imageBatch.shape}\")\n",
    "\n",
    "# apply data augmentation\n",
    "dataAugmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look at the data again\n",
    "To visualize the effects of the data augmentation an example image is shown after being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data augmentation\n",
    "plt.figure(figsize=(10, 10))\n",
    "for image, _ in trainDataset.take(1):    \n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = dataAugmentation(image)\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "13st3": [
       {
        "id": "12668441/3KVIW22P",
        "source": "zotero"
       }
      ],
      "9nj56": [
       {
        "id": "12668441/HSDR2DAU",
        "source": "zotero"
       }
      ],
      "d52is": [
       {
        "id": "12668441/3KVIW22P",
        "source": "zotero"
       }
      ],
      "wolwk": [
       {
        "id": "12668441/QSWZ8C7A",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "### Transfer Learning\n",
    "A common approach to do image classification tasks with deep neural networks is transfer learning. For transfer learning a pre-trained model is used as the base of the neural network. Then the model is adapted and fine-tuned to work on the specific task. The biggest advantage of transfer learning in image classification is that it can be applied to smaller datasets and the model trains faster. <cite id=\"wolwk\"><a href=\"#zotero%7C12668441%2FQSWZ8C7A\">(Trofimov &#38; Bogatyreva, 2020)</a></cite> \n",
    "\n",
    "The Keras framework provides a variety of available pre-trained models usable for transfer learning. In the following, two suitable models will be determined and subsequently trained. The results of the training will then be analyzed and discussed. The table below shows the models to choose from. Where the top-1 and top-5 accuracy was determined based on the performance of the model based on the [ImageNet](https://www.image-net.org/index.php) dataset. The value for the depth of the network is composed of the number of activation layers, batch normalization layers etc. Time per inference step is the average of 30 batches and 10 repetitions on the hardware used by Keras and varies on every machine. <cite id=\"13st3\"><a href=\"#zotero%7C12668441%2F3KVIW22P\">(Keras Team, n.d.)</a></cite>\n",
    "\n",
    "| Model             | Size (MB) | Top-1 Accuracy | Top-5 Accuracy | Parameters | Depth | Time (ms) per inference step (CPU) | Time (ms) per inference step (GPU) |\n",
    "|-------------------|-----------|----------------|----------------|------------|-------|------------------------------------|------------------------------------|\n",
    "| Xception          | 88        | 79.0%          | 94.5%          | 22.9M      | 81    | 109.4                              | 8.1                                |\n",
    "| VGG16             | 528       | 71.3%          | 90.1%          | 138.4M     | 16    | 69.5                               | 4.2                                |\n",
    "| VGG19             | 549       | 71.3%          | 90.0%          | 143.7M     | 19    | 84.8                               | 4.4                                |\n",
    "| ResNet50          | 98        | 74.9%          | 92.1%          | 25.6M      | 107   | 58.2                               | 4.6                                |\n",
    "| ResNet50V2        | 98        | 76.0%          | 93.0%          | 25.6M      | 103   | 45.6                               | 4.4                                |\n",
    "| ResNet101         | 171       | 76.4%          | 92.8%          | 44.7M      | 209   | 89.6                               | 5.2                                |\n",
    "| ResNet101V2       | 171       | 77.2%          | 93.8%          | 44.7M      | 205   | 72.7                               | 5.4                                |\n",
    "| ResNet152         | 232       | 76.6%          | 93.1%          | 60.4M      | 311   | 127.4                              | 6.5                                |\n",
    "| ResNet152V2       | 232       | 78.0%          | 94.2%          | 60.4M      | 307   | 107.5                              | 6.6                                |\n",
    "| InceptionV3       | 92        | 77.9%          | 93.7%          | 23.9M      | 189   | 42.2                               | 6.9                                |\n",
    "| InceptionResNetV2 | 215       | 80.3%          | 95.3%          | 55.9M      | 449   | 130.2                              | 10.0                               |\n",
    "| MobileNet         | 16        | 70.4%          | 89.5%          | 4.3M       | 55    | 22.6                               | 3.4                                |\n",
    "| MobileNetV2       | 14        | 71.3%          | 90.1%          | 3.5M       | 105   | 25.9                               | 3.8                                |\n",
    "| DenseNet121       | 33        | 75.0%          | 92.3%          | 8.1M       | 242   | 77.1                               | 5.4                                |\n",
    "| DenseNet169       | 57        | 76.2%          | 93.2%          | 14.3M      | 338   | 96.4                               | 6.3                                |\n",
    "| DenseNet201       | 80        | 77.3%          | 93.6%          | 20.2M      | 402   | 127.2                              | 6.7                                |\n",
    "| NASNetMobile      | 23        | 74.4%          | 91.9%          | 5.3M       | 389   | 27.0                               | 6.7                                |\n",
    "| NASNetLarge       | 343       | 82.5%          | 96.0%          | 88.9M      | 533   | 344.5                              | 20.0                               |\n",
    "| EfficientNetB0    | 29        | 77.1%          | 93.3%          | 5.3M       | 132   | 46.0                               | 4.9                                |\n",
    "| EfficientNetB1    | 31        | 79.1%          | 94.4%          | 7.9M       | 186   | 60.2                               | 5.6                                |\n",
    "| EfficientNetB2    | 36        | 80.1%          | 94.9%          | 9.2M       | 186   | 80.8                               | 6.5                                |\n",
    "| EfficientNetB3    | 48        | 81.6%          | 95.7%          | 12.3M      | 210   | 140.0                              | 8.8                                |\n",
    "| EfficientNetB4    | 75        | 82.9%          | 96.4%          | 19.5M      | 258   | 308.3                              | 15.1                               |\n",
    "| EfficientNetB5    | 118       | 83.6%          | 96.7%          | 30.6M      | 312   | 579.2                              | 25.3                               |\n",
    "| EfficientNetB6    | 166       | 84.0%          | 96.8%          | 43.3M      | 360   | 958.1                              | 40.4                               |\n",
    "| EfficientNetB7    | 256       | 84.3%          | 97.0%          | 66.7M      | 438   | 1578.9                             | 61.6                               |\n",
    "| EfficientNetV2B0  | 29        | 78.7%          | 94.3%          | 7.2M       | -     | -                                  | -                                  |\n",
    "| EfficientNetV2B1  | 34        | 79.8%          | 95.0%          | 8.2M       | -     | -                                  | -                                  |\n",
    "| EfficientNetV2B2  | 42        | 80.5%          | 95.1%          | 10.2M      | -     | -                                  | -                                  |\n",
    "| EfficientNetV2B3  | 59        | 82.0%          | 95.8%          | 14.5M      | -     | -                                  | -                                  |\n",
    "| EfficientNetV2S   | 88        | 83.9%          | 96.7%          | 21.6M      | -     | -                                  | -                                  |\n",
    "| EfficientNetV2M   | 220       | 85.3%          | 97.4%          | 54.4M      | -     | -                                  | -                                  |\n",
    "| EfficientNetV2L   | 479       | 85.7%          | 97.5%          | 119.0M     | -     | -                                  | -                                  |\n",
    "\n",
    "<div align=\"center\">Table: Models available in Keras. <cite id=\"d52is\"><a href=\"#zotero%7C12668441%2F3KVIW22P\">(Keras Team, n.d.)</a></cite></div>\n",
    "<br />\n",
    "The table shows that the time required for an inference step is directly related to the number of parameters and the depth of the network. In order to find the right network for the respective application, the available hardware should be taken into account and, depending on the hardware, networks with few parameters or less depth should be used for faster training. \n",
    "\n",
    "Accuracy should also be taken into account. A distinction should be made between top-1 and top-5 accuracy. The top-1 accuracy is that the prediction of the model is exactly the expected answer. Top-5 accuracy means that the model determines the correct answer among the 5 highest probabilities of the network prediction. <cite id=\"9nj56\"><a href=\"#zotero%7C12668441%2FHSDR2DAU\">(Dang, 2021)</a></cite> \n",
    "\n",
    "Since a strong graphics card, the [Nvidia GeForce RTX 3070](https://www.nvidia.com/de-de/geforce/graphics-cards/30-series/rtx-3070-3070ti/), is available for training, a computationally intensive model will be trained. In addition, the model should have a high top-1 accuracy, since it should distinguish exactly between my face and other faces or objects. For comparison, a model with lower hardware requirements is trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "3in4f": [
       {
        "id": "12668441/MVDHUU4A",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "#### Transfer learning workflow\n",
    "A common transfer learning approach is to freeze the layers of the pre-trained model. That means the weights of the base model won't be updated to avoid trainig the model from scratch. Next there are new layers added on top of the forzen layers which are trainable. These layers are necessary to turn the features of the pre-trained model into predictions of the new dataset. Then the model is trained. Optionally the model can be fine-tuned by making all the layers trainable and and adapting them to the new data. <cite id=\"3in4f\"><a href=\"#zotero%7C12668441%2FMVDHUU4A\">(<i>Transfer Learning and Fine-Tuning | TensorFlow Core</i>, n.d.)</a></cite>\n",
    "This transfer learning workflow is shown for both base models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "initialEpochs = 100\n",
    "baseLearningRate = 0.001\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "imgSize = (224, 224)\n",
    "imgShape = imgSize + (3,)\n",
    "print(f\"Image shape: {imgShape}\")\n",
    "baseModel = tf.keras.applications.MobileNetV2(input_shape=imgShape,\n",
    "                                              include_top=False,\n",
    "                                              weights='imagenet')\n",
    "\n",
    "imageBatch, labelBatch = next(iter(trainDataset))\n",
    "featureBatch = baseModel(imageBatch)\n",
    "print(f\"Feature batch shape: {featureBatch.shape}\")\n",
    "\n",
    "# freeze the convolutional base\n",
    "baseModel.trainable = False\n",
    "\n",
    "# show model\n",
    "# baseModel.summary()\n",
    "numClasses = len(classNames)\n",
    "print(f\"Sum of classes: {numClasses}\")\n",
    "\n",
    "inputs = tf.keras.Input(shape=imgShape)\n",
    "x = dataAugmentation(inputs)\n",
    "x = preprocessInputMobileNetV2(x)\n",
    "x = rescale(x)\n",
    "x = baseModel(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "x = tf.keras.layers.Dense(units=16)(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(numClasses, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=baseLearningRate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# loss0, accuracy0 = model.evaluate(validationDataset)\n",
    "# print(\"initial loss: {:.2f}\".format(loss0))\n",
    "# print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "initialEpochs = 20\n",
    "baseLearningRate = 0.0001\n",
    "\n",
    "# Create the base model from the pre-trained model Xception\n",
    "imgSize = (224, 224)\n",
    "imgShape = imgSize + (3,)\n",
    "print(imgShape)\n",
    "baseModel = tf.keras.applications.Xception(input_shape=imgShape,\n",
    "                                              include_top=False, \n",
    "                                              weights='imagenet')\n",
    "\n",
    "imageBatch, labelBatch = next(iter(trainDataset))\n",
    "featureBatch = baseModel(imageBatch)\n",
    "print(featureBatch.shape)\n",
    "\n",
    "# freeze the convolutional base\n",
    "baseModel.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=imgShape)\n",
    "x = preprocessInputXception(x)\n",
    "x = dataAugmentation(inputs)\n",
    "x = rescale(x)\n",
    "x = baseModel(x, training=False)\n",
    "x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(numClasses, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# # show model\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=baseLearningRate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath_xception = \"/checkpoints/xception\"\n",
    "checkpoint_filepath_mobileNetV2 = \"/checkpoints/mobileNetV2\"\n",
    "\n",
    "xception_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_xception,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "mobileNetV2_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_mobileNetV2,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyXception = model.fit(trainDataset,\n",
    "                    epochs=initialEpochs,\n",
    "                    validation_data=validationDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyMobileNetV2 = model.fit(trainDataset,\n",
    "                    epochs=initialEpochs,\n",
    "                    validation_data=validationDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad-CAM class activation visualization\n",
    "\n",
    "Vergleichen mit OpenCV zugeschnitten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingHistory(model_learning, label, save_fig = True):\n",
    "    # Basic plot configuration\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    fig.suptitle('Training history: ' + label, fontsize=16, y=1.01, fontweight='bold')\n",
    "    # Loss\n",
    "    plt.subplot(121)\n",
    "    plt.plot(model_learning.history['loss'], label='train loss')\n",
    "    plt.plot(model_learning.history['val_loss'], label='val loss')\n",
    "    plt.ylim(0,1.0)\n",
    "    plt.legend()\n",
    "    # Accuracies\n",
    "    plt.subplot(122)\n",
    "    plt.plot(model_learning.history['accuracy'], label='train accuracy')\n",
    "    plt.plot(model_learning.history['val_accuracy'], label='val accuracy')\n",
    "    plt.ylim(0,1.15)\n",
    "    plt.legend()\n",
    "    # Save figure as .png\n",
    "    if save_fig:\n",
    "        plt.savefig('./figures/' + label + '_TrainingHistory.png', bbox_inches='tight', transparent = True)\n",
    "        print('Figure saved: '+'./figures/' + label + '_TrainingHistory.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history: Xception\n",
    "loadWeights = False\n",
    "if not loadWeights:\n",
    "    plotTrainingHistory(historyXception, 'Xception', save_fig = False)\n",
    "else:\n",
    "    # Load already saved image of current model's training history\n",
    "    Xception_HistIMG = plt.imread('./figures/Xception_TrainingHistory.png')\n",
    "\n",
    "    fig = plt.figure(figsize=(17,6))\n",
    "    plt.imshow(Xception_HistIMG)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadWeights = False\n",
    "if not loadWeights:\n",
    "    plotTrainingHistory(historyMobileNetV2, 'MobileNetV2', save_fig = False)\n",
    "else:\n",
    "    # Load already saved image of current model's training history\n",
    "    MobileNetV2_HistIMG = plt.imread('./figures/MobileNetV2_TrainingHistory.png')\n",
    "\n",
    "    fig = plt.figure(figsize=(17,6))\n",
    "    plt.imshow(MobileNetV2_HistIMG)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "<!-- BIBLIOGRAPHY START -->\n",
    "<div class=\"csl-bib-body\">\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/TCJXDMGP\"></i><i>Better performance with the tf.data API | TensorFlow Core</i>. (n.d.). TensorFlow. Retrieved July 10, 2022, from <a href=\"https://www.tensorflow.org/guide/data_performance\">https://www.tensorflow.org/guide/data_performance</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/EXM36PCP\"></i>Bradski, G. (2000). The OpenCV Library. <i>Dr. Dobb’s Journal of Software Tools</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/HSDR2DAU\"></i>Dang, A. T. (2021, January 7). <i>Accuracy and Loss: Things to Know about The Top 1 and Top 5 Accuracy</i>. Medium. <a href=\"https://towardsdatascience.com/accuracy-and-loss-things-to-know-about-the-top-1-and-top-5-accuracy-1d6beb8f6df3\">https://towardsdatascience.com/accuracy-and-loss-things-to-know-about-the-top-1-and-top-5-accuracy-1d6beb8f6df3</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/K8WZ388C\"></i><i>Data augmentation | TensorFlow Core</i>. (n.d.). TensorFlow. Retrieved June 19, 2022, from <a href=\"https://www.tensorflow.org/tutorials/images/data_augmentation\">https://www.tensorflow.org/tutorials/images/data_augmentation</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/AN5HZ4B7\"></i>Huang, G. B., Ramesh, M., Berg, T., &#38; Learned-Miller, E. (2007). <i>Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments</i> (No. 07–49). University of Massachusetts, Amherst. <a href=\"http://vis-www.cs.umass.edu/lfw/index.html\">http://vis-www.cs.umass.edu/lfw/index.html</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/3KVIW22P\"></i>Keras Team. (n.d.). <i>Keras documentation: Keras Applications</i>. Retrieved July 13, 2022, from <a href=\"https://keras.io/api/applications/\">https://keras.io/api/applications/</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/DW3K5WQH\"></i><i>Load and preprocess images | TensorFlow Core</i>. (n.d.). TensorFlow. Retrieved June 19, 2022, from <a href=\"https://www.tensorflow.org/tutorials/load_data/images\">https://www.tensorflow.org/tutorials/load_data/images</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/MVDHUU4A\"></i><i>Transfer learning and fine-tuning | TensorFlow Core</i>. (n.d.). TensorFlow. Retrieved July 14, 2022, from <a href=\"https://www.tensorflow.org/guide/keras/transfer_learning\">https://www.tensorflow.org/guide/keras/transfer_learning</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|12668441/QSWZ8C7A\"></i>Trofimov, A. G., &#38; Bogatyreva, A. A. (2020). A Method of Choosing a Pre-trained Convolutional Neural Network for Transfer Learning in Image Classification Problems. In B. Kryzhanovsky, W. Dunin-Barkowski, V. Redko, &#38; Y. Tiumentsev (Eds.), <i>Advances in Neural Computation, Machine Learning, and Cognitive Research III</i> (Vol. 856, pp. 263–270). Springer International Publishing. <a href=\"https://doi.org/10.1007/978-3-030-30425-6_31\">https://doi.org/10.1007/978-3-030-30425-6_31</a></div>\n",
    "</div>\n",
    "<!-- BIBLIOGRAPHY END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {
    "zotero": {
     "12668441/3KVIW22P": {
      "URL": "https://keras.io/api/applications/",
      "abstract": "Keras documentation",
      "accessed": {
       "date-parts": [
        [
         2022,
         7,
         13
        ]
       ]
      },
      "author": [
       {
        "family": "Keras Team",
        "given": ""
       }
      ],
      "id": "12668441/3KVIW22P",
      "language": "en",
      "shortTitle": "Keras documentation",
      "system_id": "zotero|12668441/3KVIW22P",
      "title": "Keras documentation: Keras Applications",
      "type": "webpage"
     },
     "12668441/AN5HZ4B7": {
      "URL": "http://vis-www.cs.umass.edu/lfw/index.html",
      "author": [
       {
        "family": "Huang",
        "given": "Gary B."
       },
       {
        "family": "Ramesh",
        "given": "Manu"
       },
       {
        "family": "Berg",
        "given": "Tamara"
       },
       {
        "family": "Learned-Miller",
        "given": "Erik"
       }
      ],
      "id": "12668441/AN5HZ4B7",
      "issued": {
       "date-parts": [
        [
         "2007",
         10
        ]
       ]
      },
      "number": "07-49",
      "publisher": "University of Massachusetts, Amherst",
      "system_id": "zotero|12668441/AN5HZ4B7",
      "title": "Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments",
      "type": "report"
     },
     "12668441/DW3K5WQH": {
      "URL": "https://www.tensorflow.org/tutorials/load_data/images",
      "accessed": {
       "date-parts": [
        [
         2022,
         6,
         19
        ]
       ]
      },
      "container-title": "TensorFlow",
      "id": "12668441/DW3K5WQH",
      "language": "en",
      "system_id": "zotero|12668441/DW3K5WQH",
      "title": "Load and preprocess images | TensorFlow Core",
      "type": "webpage"
     },
     "12668441/EXM36PCP": {
      "abstract": "OpenCV provides a real-time optimized Computer Vision library, tools, and hardware. It also supports model execution for Machine Learning (ML) and Artificial Intelligence (AI).",
      "author": [
       {
        "family": "Bradski",
        "given": "G."
       }
      ],
      "container-title": "Dr. Dobb's Journal of Software Tools",
      "id": "12668441/EXM36PCP",
      "issued": {
       "date-parts": [
        [
         2000
        ]
       ]
      },
      "language": "en-US",
      "system_id": "zotero|12668441/EXM36PCP",
      "title": "The OpenCV Library",
      "type": "article-journal"
     },
     "12668441/HSDR2DAU": {
      "URL": "https://towardsdatascience.com/accuracy-and-loss-things-to-know-about-the-top-1-and-top-5-accuracy-1d6beb8f6df3",
      "abstract": "Measure the performance of our model",
      "accessed": {
       "date-parts": [
        [
         2022,
         7,
         13
        ]
       ]
      },
      "author": [
       {
        "family": "Dang",
        "given": "Anh T."
       }
      ],
      "container-title": "Medium",
      "id": "12668441/HSDR2DAU",
      "issued": {
       "date-parts": [
        [
         2021,
         1,
         7
        ]
       ]
      },
      "language": "en",
      "shortTitle": "Accuracy and Loss",
      "system_id": "zotero|12668441/HSDR2DAU",
      "title": "Accuracy and Loss: Things to Know about The Top 1 and Top 5 Accuracy",
      "type": "webpage"
     },
     "12668441/K8WZ388C": {
      "URL": "https://www.tensorflow.org/tutorials/images/data_augmentation",
      "accessed": {
       "date-parts": [
        [
         2022,
         6,
         19
        ]
       ]
      },
      "container-title": "TensorFlow",
      "id": "12668441/K8WZ388C",
      "language": "en",
      "system_id": "zotero|12668441/K8WZ388C",
      "title": "Data augmentation | TensorFlow Core",
      "type": "webpage"
     },
     "12668441/MVDHUU4A": {
      "URL": "https://www.tensorflow.org/guide/keras/transfer_learning",
      "accessed": {
       "date-parts": [
        [
         2022,
         7,
         14
        ]
       ]
      },
      "container-title": "TensorFlow",
      "id": "12668441/MVDHUU4A",
      "language": "en",
      "system_id": "zotero|12668441/MVDHUU4A",
      "title": "Transfer learning and fine-tuning | TensorFlow Core",
      "type": "webpage"
     },
     "12668441/QSWZ8C7A": {
      "ISBN": "9783030304249 9783030304256",
      "URL": "http://link.springer.com/10.1007/978-3-030-30425-6_31",
      "accessed": {
       "date-parts": [
        [
         2022,
         7,
         13
        ]
       ]
      },
      "author": [
       {
        "family": "Trofimov",
        "given": "Alexander G."
       },
       {
        "family": "Bogatyreva",
        "given": "Anastasia A."
       }
      ],
      "container-title": "Advances in Neural Computation, Machine Learning, and Cognitive Research III",
      "editor": [
       {
        "family": "Kryzhanovsky",
        "given": "Boris"
       },
       {
        "family": "Dunin-Barkowski",
        "given": "Witali"
       },
       {
        "family": "Redko",
        "given": "Vladimir"
       },
       {
        "family": "Tiumentsev",
        "given": "Yury"
       }
      ],
      "event-place": "Cham",
      "id": "12668441/QSWZ8C7A",
      "issued": {
       "date-parts": [
        [
         2020
        ]
       ]
      },
      "language": "en",
      "note": "DOI: 10.1007/978-3-030-30425-6_31",
      "page": "263-270",
      "publisher": "Springer International Publishing",
      "publisher-place": "Cham",
      "system_id": "zotero|12668441/QSWZ8C7A",
      "title": "A Method of Choosing a Pre-trained Convolutional Neural Network for Transfer Learning in Image Classification Problems",
      "type": "chapter",
      "volume": "856"
     },
     "12668441/TCJXDMGP": {
      "URL": "https://www.tensorflow.org/guide/data_performance",
      "accessed": {
       "date-parts": [
        [
         2022,
         7,
         10
        ]
       ]
      },
      "container-title": "TensorFlow",
      "id": "12668441/TCJXDMGP",
      "language": "en",
      "system_id": "zotero|12668441/TCJXDMGP",
      "title": "Better performance with the tf.data API | TensorFlow Core",
      "type": "webpage"
     }
    }
   }
  },
  "cite2c": {
   "citations": {
    "12668441/5BZPIWDZ": {
     "URL": "https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub",
     "accessed": {
      "date-parts": [
       [
        2022,
        6,
        18
       ]
      ]
     },
     "container-title": "TensorFlow",
     "language": "en",
     "title": "Transfer learning with TensorFlow Hub | TensorFlow Core",
     "type": "webpage"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e1b1a4de2e6faa058f1af7ccadcb739fc63e302212f431f62714287f55ca948"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
